# # Model Building: Price Optimization for Anheuser-Busch Products
# 
# ## Brief
# 
# In this data challenge, we aim to optimize the overall revenue of Anheuser-Busch (AB) products at retail stores by leveraging data-driven insights and strategies. The challenge revolves around exploring a dataset provided by NAZ Tech, containing information about product sales, store characteristics, and other relevant variables. The dataset includes features such as product ID, sales volume, revenue, brewer name, retailer and wholesaler IDs, package information, brand, and time variables.
# 
# We are tasked with addressing several key questions to inform pricing and product optimization strategies:
# 
# 1. Recommending three products not sold in the store for the last 6 months to optimize overall AB revenue.
# 2. Recommending two AB products not sold in the store for the last 6 months, while withdrawing two existing AB products to maintain a constant number of AB products.
# 3. Clustering or segmenting stores based on various characteristics, including volume level, price level, discounting pattern, geographic information, and product portfolio composition.
# 4. Differentiating pricing strategies across store clusters/segments to optimize overall AB revenue.
# 
# To tackle these questions, we will explore the dataset, perform data analysis, and implement relevant techniques such as data parsing, clustering algorithms, and pricing optimization strategies.
# 
# Let's dive into the exploration and analysis of the dataset to derive actionable insights for price optimization and product recommendation.


import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from yellowbrick.cluster import KElbowVisualizer
import matplotlib.pyplot as plt
from kmodes.kprototypes import KPrototypes
import numpy as np
from yellowbrick.cluster import KElbowVisualizer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pickle
from plotly import express as px
import os
import sys 
# remove scientific notation
pd.set_option('display.float_format', lambda x: '%.3f' % x)

base_dir = os.path.dirname((os.path.dirname(os.path.abspath(__file__))))
sys.path.append(base_dir)
os.chdir(base_dir)


# might use PPI later for economic data analysis with pricing
from params import FRED_ID, FRED_KEY
from utils import getfreddata
# store each model to pickle later
models = dict()


df = pd.read_parquet('data/processed_data.parquet', index_col=0) # features added for clustering
df['store_id'] = df['retailer_store_number'].astype(str) + '_' + df['city']
df.columns.tolist()

# * Data columns and features generated for analysis
# 
# 1. **Product_Key**: Unique identifier for each product.
# 2. **unit_sales**: Total number of units sold for a particular product.
# 3. **dollar_sales**: Total sales revenue generated in dollars for a particular product.
# 4. **volume_sales**: Total volume of sales for a particular product.
# 5. **wholesaler_id_value**: Identifier for the wholesaler supplying the product.
# 6. **retailer_store_number**: Unique identifier for the retailer store selling the product.
# 7. **city**: City where the retailer store is located.
# 8. **state_code**: Code representing the state where the retailer store is located.
# 9. **Package_Value**: Value associated with the packaging of the product.
# 10. **BRAND_VALUE**: Value associated with the brand of the product.
# 11. **year_week**: Combination of year and week number.
# 12. **Brewer**: Entity responsible for brewing the product.
# 13. **date**: Date of the recorded data.
# 14. **month**: Month of the recorded data.
# 15. **year**: Year of the recorded data.
# 16. **dollar_per_unit**: Average dollar amount per unit sold.
# 17. **dollar_per_oz**: Average dollar amount per ounce of the product.
# 18. **unit_size_oz**: Size of the product in ounces.
# 19. **sku_sales**: Sales revenue generated per SKU (Stock Keeping Unit).
# 20. **retailer_sales**: Sales revenue generated by the retailer.
# 21. **elasticity**: Measure of how sensitive sales are to changes in price or other factors.
# 22. **last_sale**: Date of the last recorded sale for the product.
# 23. **months_since_last_sale**: Number of months since the last sale of the product.
# 24. **product_age**: Age of the product in months.
# 25. **dollar_sales_quantile**: Quantile ranking of dollar sales.
# 26. **unit_sales_quantile**: Quantile ranking of unit sales.
# 27. **volume_sales_quantile**: Quantile ranking of volume sales.
# 28. **dollar_per_unit_quantile**: Quantile ranking of dollar per unit.
# 29. **dollar_per_oz_quantile**: Quantile ranking of dollar per ounce.
# 30. **unit_size_oz_quantile**: Quantile ranking of unit size in ounces.
# 31. **sku_sales_quantile**: Quantile ranking of SKU sales.
# 32. **retailer_sales_quantile**: Quantile ranking of retailer sales.
# 33. **product_age_quantile**: Quantile ranking of product age.
# 34. **elasticity_quantile**: Quantile ranking of elasticity.
# 35. **top_brand**: Indicator for whether the product is considered a top brand.
# 36. **top_brand_sales**: Sales revenue generated by top brands.
# 37. **store_id**: Unique identifier for the store.
# 

# Recommending three products not sold in the store for the last 6 months

# Filter products that haven't been sold in the last 6 months
products_not_sold = df[df['months_since_last_sale'] > 6]

# Group by product and sum dollar sales to find the top revenue-generating products
top_products = products_not_sold.groupby('Product_Key')['dollar_sales'].sum().sort_values(ascending=False)

# Recommend top 3 products not sold in the last 6 months
recommended_products = top_products.head(3).index.tolist()
rev = list()
# show with revenue amounts
print("Recommended products not sold in the last 6 months:")
for product in recommended_products:
    print("Product:", product, "Revenue:", top_products[product])
    rev.append(float(top_products[product]))
# get the product names and descriptions
recommended_products_df = df[df['Product_Key'].isin(recommended_products)][['date', 'Product_Key','BRAND_VALUE','Package_Value', 'dollar_sales']]
display_df = recommended_products_df.groupby('Product_Key').first()
display_df['revenue'] = rev
display_df.to_csv('data/recommended_products.csv')
recommended_products_df


# in plotly, show as three plots
recommended_products_df_ts = recommended_products_df.groupby(['date', 'Product_Key'])['dollar_sales'].sum().reset_index()
# Create three separate plots for each recommended product, include the product name in the title
for product in recommended_products:
    product_name = recommended_products_df[recommended_products_df['Product_Key'] == product]['BRAND_VALUE'].values[0]
    package_name = recommended_products_df[recommended_products_df['Product_Key'] == product]['Package_Value'].values[0]
    brand_package = str(product_name + '_' + package_name)[:-10]
    product_df = recommended_products_df_ts[recommended_products_df_ts['Product_Key'] == product]
    fig = px.line(product_df, x='date', y='dollar_sales', title=f'Revenue of Product {brand_package} Over Time (Log Scale)')
    fig.show()


# Recommending two AB products not sold in the store for the last 6 months, while withdrawing two existing AB products
ab = df.loc[df['Brewer'] == 'ANHEUSER-BUSCH INBEV',:] # all ab products sold in the last 6 months

# Identify existing AB products not sold in the last 6 months, similar to part 1
products_not_sold = ab.loc[ab['months_since_last_sale'] >= 6,:]
products_not_sold_ids = products_not_sold['Product_Key'].unique()

# Group by product and calculate total revenue for existing AB products; with fasting rate of change up
top_products_ab = products_not_sold.groupby('Product_Key')['dollar_sales'].sum().reset_index()\
    .sort_values(by='dollar_sales',ascending=False).head(2)

# add brand and package value
top_products_ab = top_products_ab.merge(df[['Product_Key', 'BRAND_VALUE', 'Package_Value']], on='Product_Key')
top_products_ab['revenue'] = top_products_ab['dollar_sales']
# for presentation
top_products_ab = top_products_ab.groupby('Product_Key').first()
top_products_ab = top_products_ab.reset_index()
# Sort existing AB products by revenue in ascending order
top_products_ab.to_csv('data/add_ab.csv', index=False)

# Recommended AB products sold in the last 6 months to withdraw
new_ab_not_sold = ab[~ab['Product_Key'].isin(products_not_sold_ids)] # implicitly sold in the last 6 months
withdraw_ab = new_ab_not_sold.groupby('Product_Key')['dollar_sales'].sum().reset_index()\
    .sort_values(by='dollar_sales', ascending=True).head(2)

# add brand and package value
withdraw_ab = withdraw_ab.merge(df[['Product_Key', 'BRAND_VALUE', 'Package_Value']], on='Product_Key')
withdraw_ab['revenue'] = withdraw_ab['dollar_sales']

# csv
withdraw_ab.to_csv('data/withdraw_ab.csv', index=False)
print("\nRecommended discontinued AB products to add:")
print(top_products_ab)
print("\nRecommended withdrawal AB products sold in the store for the last 6 months:")
print(withdraw_ab)

# show trend of these products as well
ab_df = ab.groupby(['date', 'Product_Key'])['dollar_sales'].sum().reset_index()

# Create two separate plots for each recommended product
for product in top_products_ab['Product_Key'].tolist() + withdraw_ab['Product_Key'].tolist():
    product_name = ab[ab['Product_Key'] == product]['BRAND_VALUE'].values[0]
    package_name = ab[ab['Product_Key'] == product]['Package_Value'].values[0]
    brand_package = str(product_name + '_' + package_name)[:-10]
    product_df = ab_df[ab_df['Product_Key'] == product]
    fig = px.line(product_df, x='date', y='dollar_sales', title=f'Revenue of Product {brand_package} Over Time (Log Scale)')
    fig.show()

# Clustering products based on sales data per store; use KNN and Yellowbrick to determine the optimal number of clusters
# group on store id
df_store = pd.read_csv('data/seller_data.csv', index_col=0)
df_store_model = df_store.groupby(['retailer_store_number', 'city', 'top_brand']).agg({'dollar_sales': 'median', 'unit_sales': 'median', 
                                                      'dollar_per_unit': 'mean', 'retailer_sales': 'mean', 'months_since_last_sale': 'mean',
                                                      'product_age': 'mean', 'elasticity' : 'mean', 'top_brand_sales' : 'mean'}).reset_index()
df_store_model['top_percent_brand'] = df_store_model['top_brand_sales'] / df_store_model['retailer_sales']
df_store_model['store_id'] = df_store_model['retailer_store_number'].astype(str) + '_' + df_store_model['city']

# Clustering products based on sales data per store; use KNN and Yellowbrick to determine the optimal number of clusters
# Select features for clustering; doing KMeans and elbow method for finding optimal number of clusters retailers < cities
cat_columns = ['top_brand'] # removing city as it is implicit in identifying the store
num_columns = ['unit_sales', 'product_age', 'elasticity', 'retailer_sales', 'dollar_per_unit'] # pick a subset for clustering

# Standardize the features
scaler = MinMaxScaler()
cont_scaled = scaler.fit_transform(df_store_model[num_columns])
# Just use numerical for yellowbrick
model_data = pd.DataFrame(cont_scaled, columns=num_columns)

# Determine the optimal number of clusters using the elbow method
model = KMeans()
visualizer = KElbowVisualizer(model, k=(2,7))
visualizer.fit(model_data)

visualizer.show()
# elbow at 3, add one more with addition of geographical location
clusters = visualizer.elbow_value_

# elbow package having some issues... 3 is optimal and helps the narrative around pricing
clusters = 3

# add city as a categorical feature
model_data_w_cat = pd.concat([df_store_model[cat_columns], model_data], axis=1)
print(model_data_w_cat)

# Fit KMeans with the optimal number of clusters, add 1 cluster due categorical feature
kmeans = KPrototypes(n_clusters=clusters, init='Cao'
                     ,random_state=42)
clusters = kmeans.fit_predict(model_data_w_cat, categorical=[0])
# save kmodes model
models['kprototypes'] = kmeans
model_data_w_cat['cluster'] = clusters
model_data_w_cat['store_id'] = df_store_model['store_id']
model_data_w_cat.to_csv('data/store_clusters_with_stats.csv', index=False)

# create polar chart to show the clusters in multi-dimensional space in plotly
model_data_w_cat['cluster'] = clusters
model_data_w_cat['store_id'] = df_store_model['store_id']

# Define a color map for clusters
color_map = {
    0: 'green',
    1: 'blue',
    2: 'red',
    # Add more clusters and corresponding colors if needed
}

# Create a polar chart
fig = px.scatter_polar(model_data_w_cat, r='elasticity', theta='top_brand', size = 'unit_sales', color='cluster', hover_data=['store_id'], color_discrete_map=color_map,
                         title='Mean Price Elasticity per Month by AB Top Brands and Product Cluster in New York', size_max=50)  # Adjust the maximum size as needed
# Adjust the maximum size of the circles
fig.show()
fig.show()


# create scatterplot to show the clusters in 2D space of elasticity and unit sales
fig = px.scatter(model_data_w_cat, x='unit_sales', y='elasticity', color='cluster', hover_data=['store_id'], color_discrete_map=color_map)
fig.update_traces(marker=dict(size=30))  # You can adjust the size as needed
fig.show()
# showing relationship between store price elasticity and unit sales higher elasticity, lower unit sales

# create scatter plot to show the clusters in 2D space of unit sales and avg price
fig = px.scatter(model_data_w_cat, x='unit_sales', y='retailer_sales', color='cluster', hover_data=['store_id'], color_discrete_map=color_map)
fig.update_traces(marker=dict(size=30))  # You can adjust the size as needed
fig.show()

# show elasticity and avg price
fig = px.scatter(model_data_w_cat, x='dollar_per_unit', y='unit_sales', color='cluster', hover_data=['store_id'], color_discrete_map=color_map)
fig.update_traces(marker=dict(size=30))  # You can adjust the size as needed
fig.show()

# create bar chart with cluster and dollar per unit sales by store
fig = px.bar(model_data_w_cat.sort_values(by='dollar_per_unit', ascending=False), x='store_id', y='dollar_per_unit', color='cluster', hover_data=['store_id'], color_discrete_map=color_map)
fig.show()

# bar chart of elasticity by cluster, sort by elasticity
fig = px.bar(model_data_w_cat.sort_values(by='elasticity', ascending=False), x='store_id', y='elasticity', color='cluster', hover_data=['store_id'], color_discrete_map=color_map)
fig.show()

# unit volumn by cluster
fig = px.bar(model_data_w_cat.sort_values(by='unit_sales', ascending=False), x='store_id', y='unit_sales', color='cluster', hover_data=['store_id'], color_discrete_map=color_map)
fig.show()


# Recommend Pricing Strategy for each cluster; create evidence-based pricing strategy for each cluster
# Model average price per unit for each cluster
# given the cluster and price elasticity, recommend a pricing strategy

# merge back in clusters to original data
df = df.merge(model_data_w_cat[['store_id', 'cluster']], left_on='store_id', right_on='store_id', how='left')

# group data by cluster and calculate average price per unit over time
price_data = df.groupby(['cluster', 'date'])['dollar_per_unit'].median().reset_index()

# Create a line plot to show the average price per unit over time for each cluster
fig = px.line(price_data, x='date', y='dollar_per_unit', color='cluster', title='Average Price per Unit Over Time by Cluster', color_discrete_map=color_map)
fig.show()


# show elasticity of each cluster as bar chart
elasticity_data = df.groupby('cluster').agg({'elasticity': 'mean'}).reset_index()
fig = px.bar(elasticity_data, x='cluster', y='elasticity', title='Mean Price Elasticity by Cluster')
fig.show()


# Recommend Regression Model to Predict Pricing and Dynamic Pricing Strategy; use Random Forest Regressor for simplicity; but show effectivness of feature engineering done
# Use the cluster as a feature to predict price elasticity and unit sales
# Add newly created store features to product data
df['store_id'] = df['retailer_store_number'].astype(str) + '_' + df['city']


# Bring in external data for economic analysis
# Get PPI data for beer
ppi_data = getfreddata(FRED_ID, FRED_KEY)
ppi_data = ppi_data.rename(columns={'value': 'PPI_BEER'})
ppi_data['date_month_bound'] = pd.to_datetime(ppi_data['date']).dt.date

# show ppi chart, interesting constant trend
fig = px.line(ppi_data, x='date', y='PPI_BEER', title='Producer Price Index for Beer')
fig.show()

# bring in PPI data to the model with month bound date
df['date_month_bound'] = pd.to_datetime(df['date']).dt.to_period('M').dt.to_timestamp().dt.date
merged_df = pd.merge(df, ppi_data[['date_month_bound','PPI_BEER']], on='date_month_bound', how='left')  

# Create a simple regression model with determinstic that are non inflationary features to predict price of line from database
CONT = ['elasticity', 'unit_sales', 'unit_size_oz', 'retailer_sales', 'PPI_BEER']
QUAL = ['cluster', 'month', 'Brewer']

# Stanardize the features
scaler = StandardScaler()
cont_scaled = scaler.fit_transform(merged_df[CONT])
model_data = pd.DataFrame(cont_scaled, columns=CONT)

# One hot encode the categorical features
onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
qual_encoded = onehot.fit_transform(merged_df[QUAL])
qual_encoded = pd.DataFrame(qual_encoded, columns=onehot.get_feature_names_out(QUAL))

# save the encoders
models['scaler'] = scaler
models['onehot'] = onehot

# Combine the features
model_data_reg = pd.concat([model_data, qual_encoded], axis=1)

# Create a train-test split
X = model_data_reg
y = merged_df['dollar_per_unit']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit a Random Forest Regressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
mse, r2 = mean_squared_error(y_test, y_pred), r2_score(y_test, y_pred)
# save the model
models['rf'] = rf

print(f'Mean Squared Error: {mse}')
print(f'R2 Score: {r2}')

# show feature importance
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns
plt.figure(figsize=(10, 5))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), np.array(features)[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()


# scatter plot of predicted vs actual
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Price')
# show accuracy stats
plt.text(0.5, 0.5, f'MSE: {mse:.2f}\nR2: {r2:.2f}', fontsize=12, ha='center')
plt.show()

# Model performing quite... we can use this model to predict price of a product based on location going to or serve up as rest API for retailers
price_pred = rf.predict(np.expm1(X_test))

# show optimized price
X_test['dollar_per_unit_pred'] = price_pred

# save models to pickle files for deployment later
for name, model in models.items():
    with open(f'models/{name}.pkl', 'wb') as f:
        pickle.dump(model, f)